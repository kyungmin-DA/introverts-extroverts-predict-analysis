# ==============================================================================
# 0. 프로젝트 준비: 라이브러리 및 데이터 로드
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 시각화 스타일 설정
plt.style.use('ggplot')

# 데이터 로드 (전체 훈련 데이터를 사용)
# ※ 이전 단계에서 통합한 전체 훈련 데이터를 사용한다고 가정합니다.
# df = pd.read_csv('data/combined_train.csv')
# 여기서는 예시로 competition_train_df를 사용하겠습니다.
df = pd.read_csv('data/train.csv')


# ==============================================================================
# 1. 데이터 기본 정보 확인
# ==============================================================================
print("### 1. 데이터 기본 정보 확인 ###\n")
print("데이터 샘플 (상위 5개):")
display(df.head())
print("\n데이터 정보 (info):")
df.info()
print("\n기술 통계량 (describe):")
display(df.describe())


# ==============================================================================
# 2. 결측치 분석
# ==============================================================================
print("\n### 2. 결측치 분석 ###")
print("missingno를 사용한 결측치 시각화")

# 결측치 매트릭스: 데이터의 어느 부분에 결측치가 있는지 확인
msno.matrix(df)
plt.title('Missing Value Matrix', fontsize=16)
plt.show()

# 결측치 바 차트: 각 컬럼별 결측치 개수 확인
msno.bar(df)
plt.title('Missing Value Bar Chart', fontsize=16)
plt.show()

# 결론: 모든 변수에 걸쳐 결측치가 산발적으로 존재함을 확인.
# 이는 특정 응답자가 일부 문항을 누락했을 가능성을 시사함.


# ==============================================================================
# 3. 데이터 분포 시각화
# ==============================================================================
print("\n### 3. 각 변수별 데이터 분포 시각화 ###")

# id를 제외한 모든 컬럼에 대해 분포 확인
for col in df.columns:
    if col == 'id':
        continue
    
    plt.figure(figsize=(8, 5))
    if df[col].dtype != 'object':  # 숫자형 변수
        sns.histplot(df[col], kde=True, bins=20)
        plt.title(f'Distribution of {col}', fontsize=16)
    else:  # 범주형 변수
        sns.countplot(x=col, data=df, palette='viridis')
        plt.title(f'Count of {col}', fontsize=16)
    plt.show()

# 결론: Target 변수인 Personality가 다소 불균형함을 확인. (Extrovert가 더 많음)


# ==============================================================================
# 4. 상관관계 분석
# ==============================================================================
print("\n### 4. 변수 간 상관관계 분석 ###")

# 히트맵 분석을 위해 Personality 컬럼을 숫자(0/1)로 변환
df_corr = df.copy()
df_corr['Personality'] = df_corr['Personality'].map({'Introvert': 1, 'Extrovert': 0})

plt.figure(figsize=(10, 8))
sns.heatmap(df_corr.drop('id', axis=1).corr(), annot=True, fmt='.2f', cmap='Blues')
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

# 결론: 변수들이 크게 두 그룹으로 나뉘는 것을 확인.
# 1) 내향성 연관 그룹: Time_spent_Alone, Stage_fear, Drained_after_socializing
# 2) 외향성 연관 그룹: Friends_circle_size, Going_outside 등


# ==============================================================================
# 5. 데이터 구조 파악을 위한 차원 축소 실험 (PCA)
# ==============================================================================
print("\n### 5. PCA를 통한 데이터 구조 파악 ###")

# PCA는 스케일에 민감하므로, 분석 전 스케일링 진행
# (주의: 이 스케일링은 오직 PCA 분석을 위한 것이며, 실제 모델링 스케일링은 별도 진행)
df_pca = df_corr.drop(['id', 'Personality'], axis=1).dropna() # PCA는 결측치를 허용하지 않음
scaler_pca = StandardScaler()
X_scaled_pca = scaler_pca.fit_transform(df_pca)

# PCA 모델 생성 및 학습
pca = PCA(n_components=4)
X_pca = pca.fit_transform(X_scaled_pca)

# 누적 설명 분산 확인
print(f"4개 주성분의 누적 설명 분산량: {np.sum(pca.explained_variance_ratio_):.2f}")

# 2D 바이플롯 시각화
plt.figure(figsize=(12, 9))
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)
for i, feature in enumerate(df_pca.columns):
    plt.arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3,
              color='r', head_width=0.1)
    plt.text(pca.components_[0, i]*3.2, pca.components_[1, i]*3.2,
             feature, color='r', fontsize=12)

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
plt.title('PCA Biplot', fontsize=16)
plt.grid()
plt.show()

# 결론: 바이플롯을 통해 데이터가 PC1 축을 기준으로 명확히 두 그룹으로 나뉘는 것을 시각적으로 확인.
# 이는 상관관계 분석에서 발견한 '내향성/외향성 그룹'과 일치하는 결과임.
print("\n--- EDA 노트북 종료 ---")
